
import numpy as np
from sklearn import tree
from sklearn.cross_validation import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
import graphviz

data = open ('bayes_clean_layout.txt')
X = []
label = []

for line in data:
    # for each example
    x = np.zeros(960)
    line.strip()
    line_arr = line.split(';')
    line_arr = filter(None, line_arr)
    # print line_arr[len(line_arr)-2]
    if line_arr[len(line_arr)-2] == " true":
        label.append(1.0)
    else:
        label.append(0.0)
    line_arr = line_arr[0:len(line_arr)-2]


    for data in line_arr:
        data2 = data.split(',')
        x[int(data2[1])*32 + int(data2[0])] = 1.0
        # x[int(data2[1])][int(data2[0])] = 1.0
        # print x
    X.append(x)

X_train, X_test, y_train, y_test = train_test_split( X, label, test_size = 0.3, random_state = 100)
clf = tree.DecisionTreeClassifier(criterion = "entropy", splitter = 'best', )
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print "accuracy is", accuracy_score(y_test, y_pred)*100

dot_data = tree.export_graphviz(clf)
graph = graphviz.Source(dot_data)
# graph.render("iris")
graph

'''
Layout_map = 
    [1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,  1], #0
    [1,	5,	202,202,202,202,202,202,13,	202,7,	202,202,202,202,202,1,	5,	203,203,203,203,203,13,	203,7,	203,203,203,203,203,1], #1
    [1,	21,21,21,21,202,202,202,202,202,202,202,202,202,202,202,1,	203,203,203,203,203,203,203,203,203,203,203,203,203,203,1],     #2
    [1,	21,	21,	21,	21,	202,202,202,202,202,202,202,202,202,202,202,1,	22,	22,	22,	22,	203,203,203,203,203,203,203,203,203,203,1], #3
    [1,	202,202,202,202,202,202,202,202,202,202,202,202,8,	202,202,1,	203,203,203,203,203,203,203,203,203,203,203,8,	203,203,1], #4
    [1,	202,202,202,202,202,202,202,202,202,202,202,202,202,202,202,1,	203,203,203,203,203,203,203,203,203,203,203,203,203,203,1], #5
    [1,	202,202,202,202,202,202,202,202,202,202,202,202,202,202,202,1,	203,203,203,203,203,203,203,203,203,203,203,203,203,203,1], #6
    [1,	31,	202,202,202,202,202,202,202,202,202,202,202,202,202,202,1,	32,	203,203,203,203,203,203,203,203,203,203,203,203,203,1], #7
    [1,	31,	31,	202,202,202,202,202,202,202,202,202,202,202,13,	202,1,	32,	32,	203,203,203,203,203,203,203,203,203,203,13,	203,1], #8
    [1,	31,	31,	31,	202,202,202,202,202,202,202,202,202,202,202,202,1,	32,	32,	32,	203,203,203,203,203,203,203,203,203,203,203,1], #9
    [1,	31,	31,	31,	31,	202,202,202,202,202,202,41,41,41,202,202,1,	32,	32,	32,	32,	203,203,203,203,203,203,203,203,203,32,1],      #10
    [1,	31,	31,	31,	31,	31,	202,202,202,202,202,41,	41,	41,	202,202,1,	32,	32,	32,	32,	32,	203,203,203,42,	42,	42,	203,203,32,1],  #11
    [1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	201,201,201,1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	201,201,201,1,	1,	1,1],   #12
    [1,	3,	201,201,201,201,4,	201,12,	201,201,201,201,201,201,201,46,	46,	201,201,201,201,201,201,201,201,201,201,201,201,201,1], #13
    [1,	201,201,36,	201,201,201,201,201,201,201,201,201,201,201,201,201,46,	201,201,201,201,201,201,201,201,201,201,201,201,201,1], #14
    [1,	201,36,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,1,	1,	1,	1,	201,201,1,	1,	1,	1,	1,	1,	1,	1,1],   #15
    [1,	201,36,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,1,	10,	44,	205,205,205,1,	204,204,204,204,25,	61,	204,1], #16
    [1,	201,201,201,201,201,201,201,201,201,201,201,2,	201,201,201,201,1,	44,	205,205,205,205,1,	204,204,204,204,25,	204,204,1], #17
    [1,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,1,	44,	205,205,205,205,1,	204,204,204,204,204,62,	204,1], #18
    [1,	201,201,201,201,201,201,201,201,201,201,201,26,	201,201,26,	201,1,	205,205,205,205,205,1,	204,204,204,204,35,	204,204,1], #19
    [1,	201,201,201,201,201,201,201,201,201,201,201,26,	201,201,26,	201,1,	24,	24,	205,34,	205,1,	204,204,204,204,35,	204,204,1], #20
    [1,	201,201,201,201,201,201,201,201,201,201,201,26,	201,201,26,	201,1,	9,	24,	205,101,34,	1,	204,204,204,204,204,204,204,1], #21
    [1,	201,201,201,201,201,201,201,201,201,201,201,26,	201,201,26,	201,1,	24,	24,	205,34,	205,1,	204,204,204,204,204,63,	204,1], #22
    [1,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,1,	1,	1,	1,	1,	1,	1,	204,204,204,204,204,204,204,1], #23
    [1,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,1,	45,	45,	204,204,13,	204,204,204,204,204,204,204,204,1], #24
    [1,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,1,	204,204,204,204,204,204,204,204,204,204,204,204,204,1], #25
    [1,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,204,204,204,204,204,204,204,204,204,204,204,204,204,1], #26
    [1,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,204,204,204,204,204,204,204,204,204,204,204,204,204,1], #27
    [1,	201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,201,1,	204,204,204,204,204,204,204,204,204,204,204,204,204,1], #28
    [1,	201,201,201,1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,1]]   #29
    #0  1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30 31


'''